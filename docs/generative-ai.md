# Generative AI


## Learning Objectives

This section will help you understand:

- What generative AI is, and where you can use it
- What frontier or foundation models are and the pros & cons of using them
- Some real-world examples of generative AI in research


## Overview

Generative AI comes out of recent work in building large models. Large in both the size of the dataset they’re built from, and in terms of number of parameters in the model (i.e. how much size they take up on disk & memory they use). 

Generative AI comes from language modelling. Language modelling is the task of predicting the next word in a sentence. This is technically a supervised learning task, where the task is given a sequence of words, predict the next word. However, it doesn’t require large amounts of effort to label data, because “next word” is inherently there in the data. This model is a very powerful autocomplete model. 

Note that there’s also “masked” LM, which is predicting the missing word in the middle of a sentence, but otherwise the concepts are the same.

What’s interesting about Generative AI models is that when trained on large amounts of data, they are able to do several tasks that they weren’t explicitly trained to do. Or, they can be fine-tuned to do a task with a small amount of data - far smaller than the amount of data that’s needed to build a supervised model from scratch. 

These large ‘foundation models’ are typically trained once by an organisation with the money and resources to do so, and then used by many others. Foundation models are the basis for tools like ChatGPT. 

At the same time, we have generative AI in audio, video and image.


## Contact

If you can't find what you need

[CONTACT US :fontawesome-solid-paper-plane:](mailto:accelerate-mle@cst.cam.ac.uk){ .md-button }





