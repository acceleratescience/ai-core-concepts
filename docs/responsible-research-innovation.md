# Responsible Research and Innovation


## Learning Objectives
This section will help you understand:

- Some of the limitations of AI models
- The wider risks of AI
- Regulation that might apply

## AI model limitations

When considering AI in your research, it's important to know about some of the limitations of current AI models. These include:

Models don't have 100% **accuracy**, and so aren't appropriate for all scenarios or tasks. This needs to be taken into account in the design of the scenario you imagine AI to be used in. In safety critical tasks, they may always need human oversight. 

**Appropriateness** some tasks just aren't appropriate for AI because they are too noisy or unpredictable. Often, if it sounds too good to be true, then it is! 

The **black-box** nature of many models mean that it can be difficult to understand or interpret why AI models make the predictions that they do. For some domains, this can be challenging. 

**Hallucinations** are the name given to a specific kind of error made by language models outputting factually incorrect information. 

## Risks and Concerns

There are also a number of wider risks and concerns about the field of AI.

ML models learn from the data they are trained on, which means that they learn and reflect any **bias** that was present in the training data. This can be particularly harmful if the models were trained on data scraped from the internet, which contains societal biases - like racial and gender - that we do not want to propagate. 

The **environmental cost** of building and using ML models is in the spotlight. These models require large amounts of computing power to build and use, and hence use significant amounts of energy. Reducing the carbon footprint of AI is top of many people's mind. 

Due to the mistakes that ML models make, it's important to think about **accountability** and human oversight. Leaving ML models to make critical decisions with no oversight raises the question about who remains accountable and responsible when the system makes the wrong decision, especially in tasks where a wrong decision can cause harm. 

From a scientific research perspective, ensuring **reproducibility** is important. However, in the fast-paced field, reproducibility can be neglected. It is hard to explain the decisions of ML models, and in many cases **explainability and transparency** are important. 

ML models are trained on data, and it's very important to know the **source of data**. Much data is copyrighted, and we may need consent from the owners of that data in order to use it.

As with all technology, there is a **potential for misuse**. For researchers, the idea that their work can be misinterpreted and misused might seem like a far-off concern. However, it's important to consider at the outset how our work might be used in ways we don't support. 



## Regulation

Regulation for AI is being developed, and this is a shifting area. Always do your own research into regulations that might apply to your research. 

The first AI law to pass is the EU's AI act. This regulation has some banned uses of AI. It also carves out some high-risk applications that come with obligations if they are deployed. Most of the act doesn't apply to research, but could come into force if you move to commercialise your work.

Other jurisdictions are crafting laws and guidelines that may come into force. 



## Contact

If you can't find what you need

[CONTACT US :fontawesome-solid-paper-plane:](mailto:accelerate-mle@cst.cam.ac.uk){ .md-button }





