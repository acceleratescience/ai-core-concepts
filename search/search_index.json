{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI Core Concepts","text":"<p>Welcome to Accelerate Science's AI Core Concepts!</p> <p>AI and data-driven techniques have the potential to revolutionise research. AI is being used across the entire scientific research lifecycle - speeding up literature review, enhancing data analysis, accelerating experimentation, generating research hypotheses, simulating physical equations, and directly modelling physical phenomena. Researchers across disciplines are looking to learn more about AI and understand how they can apply it in their own work.</p> <p>This short set of online resources explain some of the core concepts behind AI. They're not a full introduction to the theory and practice of AI, but a starting point for researchers who are interested in exploring applied AI in their work.</p> <p>The first part dives into some of these AI core concepts and builds up an intuition about how AI systems work, without relying on you having any specific maths and computer programming knowledge. This will help you understand how and where AI might be useful in your research, demystify some of the field's vocabulary, and give you the basis to dig further. The second part talks about some of the practicalities and process of building AI systems, signposting ways to get started. This can help you interpret research papers in AI, and to get a sense of the work involved if you choose to go ahead and implement AI in your work.</p>"},{"location":"#part-1-core-concepts","title":"Part 1: Core Concepts","text":"<ul> <li> <p> What is AI?</p> <p>A look at what AI is, and isn't</p> </li> </ul> <ul> <li> <p> Supervised Learning</p> <p>Supervised Learning</p> </li> </ul> <ul> <li> <p> Unsupervised Learning</p> <p>Unsupervised Learning</p> </li> </ul> <ul> <li> <p> Reinforcement Learning</p> <p>Reinforcement Learning</p> </li> </ul> <ul> <li> <p> Generative AI</p> <p>Generative AI</p> </li> </ul> <ul> <li> <p> Limitations of AI</p> <p>Limitations of AI</p> </li> </ul> <ul> <li> <p> Natural Language Processing</p> <p>Natural Language Processing</p> </li> </ul> <ul> <li> <p> Computer Vision</p> <p>Computer Vision</p> </li> </ul>"},{"location":"#part-2-practical-considerations","title":"Part 2: Practical Considerations","text":"<ul> <li> <p> The AI Project Lifecycle</p> <p>The AI Project Lifecycle</p> </li> </ul> <ul> <li> <p> Data for AI Models</p> <p>Data for AI Models</p> </li> </ul> <ul> <li> <p> Training AI Models</p> <p>Training AI Models</p> </li> </ul> <ul> <li> <p> Evaluating AI Models</p> <p>Evaluating AI Models</p> </li> </ul> <ul> <li> <p> Some Practical Advice</p> <p>Some practical advice</p> </li> </ul> <ul> <li> <p> Next Steps</p> <p>Next Steps</p> </li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>If you can't find what you need, or just need more help</p> <p>CONTACT US </p>"},{"location":"ai-project-lifecycle/","title":"The AI Project Lifecycle","text":""},{"location":"ai-project-lifecycle/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>What AI projects look like in practice</li> </ul>"},{"location":"ai-project-lifecycle/#building-machine-learning-models","title":"Building Machine Learning Models","text":"<p>Building a machine learning model starts at the outset with deciding on the task that you want to apply AI to. Clarity about what you aim to achieve with AI can help smooth the project and set you on the path to a successful outcome.</p> <p>Before embarking on building AI models, it\u2019s useful to identify any non-AI methods that might also be used to tackle the task. Simpler non-AI methods can be quick to try out and can make a good baseline from which to judge the performance of your AI approach. For example, with the example of identifying toxic and non-toxic molecules, it might be feasible to write a set of deterministic rules that quickly and easily make a rough determination of toxicity. AI can then be used to improve on this simple baseline.</p> <p>At this early point in your project, it\u2019s also helpful to think about any risks and concerns with your project that ought to be addressed from the outset.</p> <p>Once you have identified a task and possibly a non-AI approach to begin with, the next step is identifying the type of AI algorithm that might be most suited for your task. From there, development in most AI projects follow the AI project lifecycle as follows:</p> <p></p> <p>Building a machine learning model involves:</p> <ol> <li>Collecting data; labelling that data, and doing any data pre-processing that\u2019s required. The data is usually split into training, validation and test sets. </li> <li>Training a model on a 'training set' of your data</li> <li>Evaluating the model on a separate 'development set', to see how well it does, looking into errors to find places for improvement. </li> </ol> <p>This tends to be an iterative process, not a linear one, so you may need to loop through these steps several times to get a model that works. It is sensible to start with simpler models and smaller datasets to get a working setup, before moving onto more complex models and large datasets. Diving straight into building a complex model on a large set can ultimately slow you down as it\u2019s difficult to get right the first time. Building up complexity from a solid foundation is generally a more successful way to approach AI. One way to iterate around this project lifecycle loop can be in using increasingly larger and larger datasets. Another way can be in using increasingly more complex models. </p> <p>Once you have iterated around this loop and converged on a model that works well, you can evaluate its final performance on a held out 'test set' to judge its real-world performance.</p> <p>While most academic AI research focuses on gaining new insights and knowledge, much research work is done with the idea of a scenario in which the model might eventually be deployed. That might perhaps be in a hospital setting or embedded into a robot. That domain may come with restrictions and limitations such as how much computing power is available or how quickly the system must respond. There may also be specific workflows that experts in that domain follow and that you aim to fit in with. As your work progresses, these concerns about deployment will become more relevant.</p> <p>It\u2019s worth noting that several different approaches may be applicable to your task. For example, identifying supernovae in astrophysics data could be framed as a supervised learning task or as an anomaly detection task. Identifying tumours in medical images could be framed as a supervised learning task or as an unsupervised clustering task. The choice of algorithm can depend on the data you have available and the relative merits of the different algorithm options available to you.</p> <p>For some fields, patient and public involvement (PPI) is important. PPI is the idea that patients should be working with researchers to drive the direction of research and how the work is disseminated. How this is implemented varies from field to field, but is increasingly important as researchers use data-driven and AI techniques. Involving participants from the outset is important.  The following sections dive into each segment - data, training &amp; evaluation - of the AI project lifecycle in depth. </p>"},{"location":"ai-project-lifecycle/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"data/","title":"Data for AI","text":""},{"location":"data/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>What data is and how it's stored</li> <li>How much data you need and how dataset size impacts your options</li> <li>Data pre-processing</li> <li>Data issues</li> <li>How researchers acquire data</li> </ul>"},{"location":"data/#working-with-data","title":"Working with data","text":"<p>To build an AI model, you'll be working closely with data. In particular, you will need a dataset consisting of many examples of the data you\u2019re working with. Data can make or break your project, and so it's crucial to think carefully about the data you're using and how it'll impact your task. It is always worth spending time early in a project to thoroughly understand your data, to visualise it, and to understand its limitations.</p> <p>The data you use for your AI work is tightly connected with what you're trying to achieve. If you're building a system that translates between languages, then your data will be text in the languages you're interested in. If you're diagnosing medical conditions in clinical settings, then your data could be medical test results, biomarkers, or medical images.</p> <p>Data can be stored in several formats. Structured data - that\u2019s data that you could list out in a table - could be stored in spreadsheets or specific file formats, like csv or json. Unstructured data like images, audio or text files are more often stored as files on your local machine, or in the cloud.</p> <p>The input of machine learning models must be numbers, not text, images or other non-numerical data type. Data is transformed into a numerical format by feature extraction, which is the step to convert your raw data into a numerical format that AI models can ingest. </p>"},{"location":"data/#data-splits","title":"Data Splits","text":"<p>Typically, when building machine learning models you will acquire a large amount of data, called a dataset. It's important to separate your large dataset into training, development, and test splits. These each have a different purpose during the building process:</p> <ul> <li>Training data is used for adjusting the model parameters during the model training phase</li> <li>Development data is used to judge the model performance at intermediate stages, and may be used for tuning hyperparameters</li> <li>Test data is used at the end of development to estimate how well your model performs on unseen data</li> </ul> <p>It is important to ensure that these sets are completely separate from each other, and there's no leakage or cross-over between training and testing data. That\u2019s to ensure the model is tested on data that it hasn\u2019t been trained on.</p> <p>If you have a dataset, then a common rule of thumb is to use 80% of data for training, 10% for development, and 10% for testing. However, you do need to be sure that 10% of data you have left for testing is enough data that you can see statistically significant differences in performance. If you have very large amounts of data, then you might choose to use more than 80% of it for training. If you have a small set, and carving out separate test and development sets leaves you with only a very small amount of training data, then a technique like k-fold cross validation may be appropriate.</p> <p>Usually, you'd create these data splits by randomly sampling your entire set. There may be cases though where you want to do something different. One example is stratified sampling, where you split your data into segments and sample from each segment to be sure you have full representation in your test set. Another example is where you have data from multiple sources and not all are appropriate test data. For example, you might choose to use synthetic data in your training set, but only real-world data in your test set. Some datasets also contain multiple examples for each subject, e.g. audio recordings taken as part of a longitudinal study, and it is usually important to ensure that those examples are kept together during the split so that data from one subject is not split across the subsets.</p> <p>Typically, data is shuffled before its used - i.e. the order is randomised. That\u2019s to ensure that any ordering that may come from the way you collected and named your data files doesn\u2019t impact the training algorithm.</p>"},{"location":"data/#how-much-data-do-you-need","title":"How much data do you need?","text":"<p>This is a commonly asked question, but unfortunately there's no hard and fast answer! More data is usually better, but sometimes the only way to know if you have enough data is to build a model and test its performance. Some of the factors that influence data needs are:</p> <p>Task: some tasks are inherently easier than others, and require less data to model.</p> <p>Model: the larger and more complex your model, the more data you need.</p> <p>Diversity: you also need diversity in your data as well as quantity. Simply adding more data that has the same information won't improve your model.</p> <p>Starting point: starting from scratch usually requires more data than if you're finetuning an already existing model.</p> <p>Data dimension: the higher the dimensionality of your data, the more of it you need to model effectively.</p> <p>You can often get a good sense of the size of datasets you need for your task by looking at related work in the literature. In general, more data tends to lead to better performing models, but more data comes with the price of an increased computational cost and longer model training times.</p>"},{"location":"data/#data-preprocessing","title":"Data Preprocessing","text":"<p>A large amount of time in any AI project is spent on acquiring, cleaning and processing data. As a scientist, you can expect to spend more time than expected on data related tasks. Data pre-processing is the work needed to prepare raw data for use by AI models.</p> <p>It's common to estimate transformations of the data, one example is data normalisation. Some features have very different ranges. For example, systolic blood pressure may be in the range 8\u2013180, while the level of hormone TSH is likely in the range 0.4 to 4.0. Using both of these features in a model can be tricky to balance. Data normalisation is where you scale the range of each feature in your data to have a specific mean and variance, typically 0 and 1.</p> <p>In some fields, when using structured data, not all of the data points are present. Perhaps a particular test is only run for a certain subset of patients, there are data entry errors, or some participants have dropped out of your study. Machine learning models require complete datasets. Missing value imputation is the task of estimating missing values in your data.</p> <p>Other common pre-processing steps include removing duplicate entries, fixing incorrect entries, increasing dataset size with data augmentation, removing outliers, selecting a subset of features, and dimensionality reduction.</p> <p>Every step in pre-processing the data is a choice, and it\u2019s important to keep records so that you can reproduce your work, and later recall what choices were made. If you are using open source data from another source, knowing what processing choices they made are important for your own scientific rigour.</p> <p>One really common way that leakage happens is during data pre-processing. For a robust setup, all of your data pre-processing should be done on the training set only. Taking the example of normalisation, estimating the transformations should be done only on the training set, and then those transformations should be applied to the development and test data. A common mistake is to estimate the transformations over the entire dataset, but this means that information from the test set will leak into your training set, via the transformations. Similar rules apply to other pre-processing operations.</p> <p>A data pipeline is a series of steps in code that you apply to your data to transform it from raw data into the form that\u2019s ready for use by your AI algorithm. </p>"},{"location":"data/#data-issues","title":"Data issues","text":"<p>Data is often the source of many problems researchers encounter when building AI models.</p> <p>A typical issue arises when there is a difference between the training and testing data that you're using. Perhaps training data was collected in one way, and test data in another. That means that your model may not work as well on your test set. One example could be a medical imaging model that is trained on images collected at one hospital, but tested on images from another where slight protocol differences mean that the data is subtly different between the two hospitals. Subtle differences like this can be hard to spot, but also highly impactful for your work. </p> <p>A similar issue arises when data distributions change over time, and is called data drift. This happens all the time in real-world scenarios! Consider the language that we use. Models trained on language from 200 years ago wouldn't perform well on today's language. And even models trained 20 years ago might miss some crucial language changes.</p> <p>Skewed data is where there is an imbalance in the amount of data between classes, because one class is far more common than another. This is common in, say, astrophysics contexts where the number of transient events of interest is low compared to the number of stable objects.</p> <p>Finally, depending on what you're doing, your data may contain personal and sensitive information about people. Keeping it safe and secure is crucial. A data management plan is a key tool for describing how you plan to manage your data. It covers factors such as how and where you can process data, where it\u2019s stored, and who can access with different levels of security, and whether it\u2019s anonymised. Your library can offer support on writing a data management plan. Sharing data beyond your group may require a data transfer agreement to be in place.</p> <p>Problems with data can sometimes be very subtle. Consider this paper that deliberately trained a poor classifier to distinguish wolves and huskies. In this setup, the images of wolves all had snow in the background, while images of huskies didn't have snow. Rather than distinguish the wolves and huskies based on their appearance, the classifier actually learned to distinguish snowy from non-snowy scenes. Tracking down these subtle data issues can take significant time and effort.</p> <p>Often, problems with machine learning models - whether they\u2019re not performing well, or are suspiciously good - come down to data. The first place to start looking for issues is in the datasets that you\u2019re using.</p>"},{"location":"data/#how-to-acquire-data","title":"How to acquire data","text":"<p>While many researchers begin with a dataset that they would like to apply AI to, others begin with the idea of a task they believe AI could be applied to and are then faced with the initial goal of acquiring data. There are several places to begin looking for relevant data:</p> <ul> <li>Experimental results: it's common to use AI methods on experimental data from your work or your lab's results.</li> <li>Web scraping: text, images and other forms of data may be available online</li> <li>Partnerships: partnering with other labs or organisations to share data can be an effective way to boost the amount of data you're working with.</li> <li>Data collection: volunteers or paid participants may be able to create the data that you require, e.g. asking for volunteers to participate in a study.</li> <li>Open source: there are many open source datasets available, which might work for your task. Many open source sets are ready split into test &amp; training sets for you to use. You need to be careful of the licence and provenance with any open source dataset. </li> <li>Synthetic: Generating synthetic data is an increasingly common way to create data.</li> <li>Data augmentation: creating noisy versions of your training data can be an effective way to boost the robustness of your models</li> </ul> <p>For supervised learning, there\u2019s also the challenge of obtaining accurate labels for the dataset. Sometimes the labels are inherently a part of the data, but other times it needs someone (researcher, volunteer, or paid participant) to spend time on labelling. Be prepared to spend significant time on labelling data if you are building up a new dataset for supervised learning. </p>"},{"location":"data/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"evaluating/","title":"Evaluating AI Models","text":""},{"location":"evaluating/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>Using test data to robustly evaluate models</li> <li>Quantitative vs qualitative evaluation</li> <li>Evaluation metrics for different types of AI algorithms</li> <li>Error analysis tools including stratification, confusion matrices &amp; ROC</li> </ul>"},{"location":"evaluating/#overview","title":"Overview","text":"<p>All machine learning models make mistakes. It\u2019s important for researchers to carefully evaluate the performance of their AI algorithms to ensure that the number of errors is within an appropriate level. Evaluation happens continuously throughout an AI project to keep on track, and also is crucial for reporting and communicating your results with the world.</p> <p>Consider a binary classification supervised model that's used to identify spam emails, and has been trained on a training dataset (circles). As the model weights have been trained using the training data, we can't use that data to see how well the classifier performs. That's a bit like seeing the exam questions ahead of time! To judge how well the classifier performs, we test its performance on a separate test set (triangles).</p> <p></p> <p>AI models are usually evaluated and compared to a baseline. Your baseline could be a non-AI approach to solving the problem, or another AI method. It is important to include one or more strong baseline systems in your experiments to be sure that your AI method is the best approach to solving the task.</p> <p>AI models are usually evaluated on a development set as models are iteratively improved during the project, and on a separate held-out test set at the end of the project. This is because continued improvements based on the development set performance leads, over time, to avoid overfitting to that dataset. By keeping a separate test set until the end of the project, you can get a better estimate of the model\u2019s performance on unseen data. </p>"},{"location":"evaluating/#qualitative-vs-quantitative-evaluation","title":"Qualitative vs Quantitative Evaluation","text":"<p>There are two main approaches to evaluate how well your model is working</p> <ol> <li>Quantitative evaluations devise an automated metric which you can automatically compute on your test set to give a score</li> <li>Qualitative evaluations ask people to interact with your model and judge it</li> </ol> <p>For some tasks, like toxicity detection, it\u2019s feasible to automatically compute an accuracy metric. For other tasks, like summarising scientific papers, there are many possible correct answers and it's hard to find a good automated metric. Hence, the way to judge performance is by having people evaluate the summaries created by the model. However, qualitative evaluations carried out by people are expensive to perform and usually require an allocated budget in your project, to pay the people who are doing the evaluation.</p> <p>In practice, researchers often lean heavily on automated quantitative evaluation metrics to evaluate their models. Those using qualitative evaluations do so on a much less frequent basis, typically only at key milestones.</p>"},{"location":"evaluating/#evaluating-supervised-learning-models","title":"Evaluating Supervised Learning Models","text":"<p>There are a wide range of metrics in use for different tasks, so the best thing to do is to look at the literature related to your task to understand how others are evaluating performance. However, some common metrics exist. For classification:</p> <ul> <li>Accuracy is simply how often the classifier is right. This is computed by comparing the classifier's prediction against the ground truth human labels in a test set.</li> <li>Precision is a measure of how many of your positive classifications were correct. For the toxicity detection case, it's the proportion of molecules the model identified as toxic that actually were toxic.</li> <li>Recall tells you what proportion of your positives were correctly identified. For the toxicity detection case, it's the proportion of actual toxic molecules in the entire test set that the model correctly identified as toxic.</li> </ul> <p>Interpreting these measures depends on the task being performed. A 95% accuracy rate may be acceptable in one task, but not in another. Accuracy can also be misleading when your data is highly skewed. For example, if your set of medical images has just 5% of images with a malignant tumour, then simply classifying all images as benign will give an accuracy of 95%.</p> <p>Precision and recall are usually reported together. An increase in one usually means a decrease in the other. For this reason, some researchers report the f1-score, which is a combination of precision and recall into a single metric.</p> <p>For regression tasks,</p> <ul> <li>RMSE root mean squared error, is a measure of how far on average your predictions are from the ground truth label.</li> </ul> <p>Researchers typically report a number of metrics to get a full picture of how their model performs. Accuracy, precision, recall and RMSE are widely used to evaluate the performance of different supervised learning models. However, there are many variants of these metrics, along with many other task-specific metrics for situations where these measures are too simple. Looking into the literature related to your specific task will be helpful to identify the specific metrics that are used in your domain. You'll need to use your judgement to select and interpret a particular metric. </p>"},{"location":"evaluating/#evaluating-unsupervised-learning-models","title":"Evaluating Unsupervised Learning Models","text":"<p>Anomaly detection problems are a type of classification, and so you may have a set of manually identified anomalies that you can use to measure accuracy, precision and recall, as is done for supervised learning.</p> <p>However, judging the performance of other unsupervised learning models is more difficult than for supervised learning, because there are often no labels to judge against. Sometimes, the result of an unsupervised learning model is the input to another algorithm, and then it is important to measure the downstream performance of that model on the task of interest. Sometimes, the result of an unsupervised learning model is a research hypothesis that must be tested experimentally. Even if that is the case, there may be multiple models and not all can be validated experimentally, so computational measures of performance are important to decide which hypotheses to take forward to test experimentally.</p> <p>Clustering is often evaluated by judging how well the proposed clusters match to the data. For each datapoint, silhouette score measures the distance between that point and other points in the same cluster, the distance between that point and points in other clusters, and computes an overall score to judge the clustering fit. Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are measures that balance the complexity of the model with goodness-of-fit, and so can help with model selection because the best model simultaneously maximises fit but minimises model complexity. Another option is to compare the result of two clustering algorithms using a score like the rand index which considers the proportion of points in the same cluster, or different clusters, in the two separate clustering results.</p> <p>Dimensionality Reduction can be evaluated by looking at how much variance in the data remains and is captured by the lower-dimensional representation. How to measure this depends on the algorithm used. For example, PCA computes eigenvalues, which can tell you the proportion of variance explained by the lower dimensional space. For cases when dimensionality reduction is an input to a supervised ML algorithm, then the performance of that model can be tested using different dimensionality reduction algorithms for input. Similarly, if used as input to a clustering algorithm then the measures of clustering performance can be examined for different numbers of low-dimensional spaces.</p> <p>In general, there are many ways to judge the performance of unsupervised learning algorithms, some of which are highly specific to the task at hand. It is always worth checking literature in your field and related work to determine if there are commonly used evaluation methods. </p>"},{"location":"evaluating/#evaluating-reinforcement-learning-models","title":"Evaluating Reinforcement Learning Models","text":"<p>It isn\u2019t usually possible to create a test set to evaluate how well a reinforcement learning algorithm performs due to the complexity of the environment they operate in and the vast number of possible actions they can take. Reinforcement learning models are typically evaluated by performing the task that they\u2019re trained to do. If there is a clear definition of whether the agent has succeeded or failed - such as a robot safely performing a specific task in a lab - then the success rate at that task can be measured. Alternatively, over a number of runs of the model in a realistic environment or simulation, the average or cumulative reward can be computed.</p> <p>Depending on the task that the agent is performing, there may be other metrics that are commonly used in your discipline. Looking at the literature and similar work in your field can help guide your evaluation.</p>"},{"location":"evaluating/#evaluating-generative-ai-models","title":"Evaluating Generative AI Models","text":"<p>AI scientists are still figuring out good ways to evaluate generative AI models, especially for more general purpose AI models that can perform multiple tasks in a zero-shot fashion.</p> <p>Evaluation metrics might aim to measure a general performance aspect of a model or its accuracy on a downstream task. For measuring general performance, large language models may measure the perplexity of the model on a held-out set of text data. This is an easy-to-compute measure of how well the model fits that text data. However, perplexity doesn\u2019t tell you anything about how useful the model is in carrying out other tasks. A downstream task might be something like multiple choice question answering, or a text classification task for which you can compute accuracy, precision and recall. Current LLMs are typically evaluated on a raft of these downstream benchmarks, across a range of tasks and domains.</p> <p>Still, these benchmarks only judge the kind of task which can be automatically scored because the correct behaviour is known. Researchers have devised automated metrics for more complex tasks, like summarisation or translation. However, these metrics have some limitations and model performance on complex tasks can only be thoroughly judged by a person. </p>"},{"location":"evaluating/#error-analysis","title":"Error Analysis","text":"<p>Usually, you 'll want to look deeper at the kind of errors that a model is making, in order to identify places where you can improve. Simply looking through the model\u2019s predictions can yield some useful insights into how the model is performing. Some other useful ways to view errors are:</p> <ul> <li>Confusion matrix a plot for classification tasks showing which categories were confused with each other.</li> <li>ROC a graph that shows classification error as you trade off precision and recall</li> <li>Stratification of your test data into different subgroups, to compute the error on those. This can help identify sub-populations with different performance, helping to identify bias in your algorithm.</li> <li>Confidence intervals can give you an insight into the likely margin of error of your predictions</li> <li>Statistical significance can tell you how likely it is that changes in your model are caused by chance, or by having an improved model.</li> </ul> <p>Error analysis can point to places where you can improve your experimental setup and achieve a better model performance. Perhaps you notice that your model doesn\u2019t work well for a specific sub-population, and you can acquire additional training data for that group. Or perhaps you identify from the confusion matrix that two categories are often confused and can make a decision about whether there should be two separate categories or whether they ought to be merged into one. </p> <p>This effort of examining the model\u2019s errors in detail forms a large part of a scientist\u2019s work. While computing metrics can give you a headline figure about the performance of your model, the time spent looking at errors can be valuable to track down improvements and build performant AI models. </p>"},{"location":"evaluating/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"generative-ai/","title":"Generative AI","text":""},{"location":"generative-ai/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>What generative AI is</li> <li>What foundation models are and the pros &amp; cons of using them</li> <li>Some real-world examples of generative AI in research</li> </ul>"},{"location":"generative-ai/#what-is-generative-ai","title":"What is Generative AI?","text":"<p>Broadly speaking, generative AI refers to models that generate content, often in the form of images, text, video or audio, but increasingly combining several content types. In the sciences, there\u2019s interest in generative AI in the design of new molecules and genetics studies.</p> <p></p> <p>In many ways, generative AI is similar to supervised learning. The key point in generative AI though is that predictions the models make are predictions about the content itself. A generative model that generates text is making predictions about the text. A generative model in the realm of drug design is making predictions about potential new molecule structures. The term  self-supervised is often used to describe these models. This is because a person isn\u2019t needed to label the data as the supervision signal is present within the data itself.</p>"},{"location":"generative-ai/#generative-ai-and-text","title":"Generative AI and Text","text":"<p>In the field of natural language processing (NLP), transformer models have had a large impact. Generative AI in NLP uses the task of language modelling - the task of predicting the next word in a sentence. This is a supervised learning task. Given a sequence of words, the model's goal is to predict the next word. It doesn\u2019t require large amounts of effort to label the text data for training because \u201cnext word\u201d is inherently there in the data. A language model is essentially a very powerful autocomplete model. Additional types of training, or finetuning, are used to turn a basic language model from a model that autocompletes text into an interactive chat model like ChatGPT.</p> <p>What\u2019s interesting about Generative AI models for NLP is that when trained on large amounts of data, they are able to do several tasks that they weren\u2019t explicitly trained to do. For example, as their training data includes a large amount of multilingual training data, they are able to translate between languages. This is described as zero shot learning.</p> <p>Text-based generative AI models typically are based on transformer models (a kind of neural network) that was proposed in 2017. Transformer-based Large Language Models (LLMs) like ChatGPT are widely available and offer researchers varied capability such as literature search, writing, summarisation and other tasks related to science reporting. They are often used for assistance with writing code, a key part of many scientists\u2019 workflows, and can potentially be used for assisting with data analysis and other research tasks. </p>"},{"location":"generative-ai/#generative-ai-and-images","title":"Generative AI and Images","text":"<p>Generative AI models involving image generation tend to make use of generative adversarial networks (GANs) or diffusion models. These are models that work by starting with images that consist of random noise, and iteratively denoising them to generate an image that matches a text description. These models find applications in scientific domains that deal with images, such as medical imaging. </p>"},{"location":"generative-ai/#generative-ai-and-scientific-domains","title":"Generative AI and Scientific Domains","text":"<p>While there\u2019s been a lot of research into generative AI models for text and image, researchers are looking too at how generative AI can be used in other fields:</p> <ul> <li>In genetics, generative AI models have been used to generate new gene editors</li> <li>In Materials Science, diffusion models have been used to accelerate the discovery of new materials that can go on to be tested, e.g. in photovoltaics to find better materials for use in solar panels.</li> <li>In Medicine, generative AI is applied to identify novel drug candidates for specific diseases.</li> <li>In Climate Science, diffusion models are applied to weather forecasting, for more accurate forecasts</li> </ul>"},{"location":"generative-ai/#foundation-models","title":"Foundation Models","text":"<p>One driver of generative AI in the text and image domain is that there\u2019s a large amount of training data for these content types widely available online, and it doesn\u2019t need to be labelled. This means that organisations are able to build larger and larger models with massive training data sets.</p> <p>Generative AI models are expensive to train, due to their large size and the large amount of training data. That means that only a handful of organisations can afford to build their own models from scratch. A typical way of working now is that these large \u2018foundation models\u2019, trained once by an organisation with the money and resources to do so, are then released publicly. Researchers can use the models directly, or fine-tune them for their own projects with a small amount of data - far smaller than the amount of data that\u2019s needed to build their own large model from scratch.</p> <p>There are some considerations to take into account when using foundation models that have been trained by an outside organisation:</p> <ul> <li>For many public models, there are varying levels of openness. Often the model weights might be available, but details about the training set may not be public. This lack of transparency can make it difficult, as a researcher, to fully know how your AI model has been trained.</li> <li>If a model is hosted by an organisation behind an API, we cannot be sure what changes they are making, and that can affect reproducibility of experiments.</li> <li>These models are typically trained on large amounts of data from the internet, which brings with it societal biases that become embedded in the model.</li> <li>There's a large environmental cost to using these models.</li> </ul>"},{"location":"generative-ai/#inspiration","title":"Inspiration","text":"<p>Here are two papers that look at applications of some of these models:</p> <ul> <li>An Interdisciplinary Outlook on Large Language Models for Scientific Research</li> <li>An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization</li> </ul>"},{"location":"generative-ai/#more-resources","title":"More Resources","text":"<p>Generative AI is a fast moving field, and details such as how to evaluate these models are open research questions. Accelerate Science has two workshops related to generative AI, each with online material that you can work through at your own pace: </p> <ul> <li>Large Language Models</li> <li>Diffusion models</li> </ul>"},{"location":"generative-ai/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"image/","title":"Image Processing / Computer Vision","text":""},{"location":"image/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>What falls under the umbrella of Computer Vision (CV) </li> <li>Some common Computer Vision tasks</li> <li>Examples of Computer Vision applied to scientific research</li> </ul>"},{"location":"image/#what-is-computer-vision","title":"What is Computer Vision?","text":"<p>Computer Vision (CV) is the umbrella term for techniques that are concerned with processing images and video. CV is a large domain, and there are many opportunities to use CV across a wide range of scientific disciplines. The field of CV has a long history making use of AI and machine learning techniques. In fact, one of the most popular introductory ML datasets you might come across is MNIST, a set of images of handwritten digits. Supervised and unsupervised learning have been applied across a range of image and video processing tasks.</p> <p>When thinking of images, we usually imagine a photo taken with a camera. However, across the sciences, there are many other sources of image data in use such as X-rays, ultrasound, CT scans, satellite images, and electron microscopy. The same techniques that are applied to photos are often used for these other image formats. Hence, if you anticipate processing any form of image or video in your research, it can be valuable to look to the CV literature to understand what has previously been done. </p> <p>Key models that you might encounter in the computer vision field include:</p> <ul> <li>Convolutional Neural Networks (CNNs) have been used successfully for many years in computer vision, and convolutional blocks form the basis of many more complex models</li> <li>Vision Transformer (ViT) is used for image classification</li> <li>Generative Adversarial Networks (GANs) are used for image generation</li> <li>Autoencoders are used for denoising</li> <li>Diffusion models, which are a pipeline consisting of multiple model types, are used to generate images</li> </ul>"},{"location":"image/#cv-tasks","title":"CV Tasks","text":"<p>Computer vision is widespread, and many CV tasks exist including:</p> <ul> <li>Image classification: classifying an image into one of several categories, e.g. classifying the handwritten digit in the MNIST task.</li> <li>Image segmentation: segmenting an image into different parts. For example, identifying boundaries of objects within an image. In a medical image this could also be something like identifying the boundaries of a tumour.</li> <li>Image generation: creating images from text descriptions, as is done by Stable Diffusion and Dall-E.</li> <li>Denoising and enhancement: removing noise from images and enhancing them.</li> <li>Pose detection: identifying the pose of the person (or animal) in an image.</li> </ul>"},{"location":"image/#examples-of-computer-vision-in-scientific-research","title":"Examples of Computer Vision in Scientific Research","text":"<p>Examples of computer vision techniques applied to scientific research include:</p> <ul> <li>In Paleontology, classifying microfossils (that\u2019s fossils &lt;1mm in size) by their type. Microfossils are more abundant than their larger macrofossil counterparts, and can tell a lot about ecosystems of the past. </li> <li>In medical imaging, segmenting a malignant mass from the rest of the image in an X-ray, Ultrasound image or CT scan.</li> <li>In Materials Science, for denoising and improving the resolution of electron microscopy images, making them easier to work with.</li> <li>In Veterinary Science, identifying the pose of animals in photographs can help with behavioural analysis.</li> </ul>"},{"location":"image/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"limitations-of-ai/","title":"Limitations of AI","text":""},{"location":"limitations-of-ai/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>Some of the limitations of AI models</li> <li>The wider risks of AI</li> <li>Policies and regulations that might apply</li> </ul>"},{"location":"limitations-of-ai/#ai-model-limitations","title":"AI model limitations","text":"<p>When considering AI in your research, it's important to know about some of the limitations of current AI models. These span across all forms of machine learning model, and include:</p> <p>Models don't have 100% accuracy, and so aren't appropriate for all scenarios or tasks. This needs to be taken into account in the design of the scenario you imagine AI to be used in. In safety critical tasks, they may always need human oversight.</p> <p>Appropriateness - some tasks just aren't appropriate for AI because they are too noisy or unpredictable. Often, if it sounds too good to be true, then it is!</p> <p>The black-box nature of many models means that it can be difficult to understand or interpret why AI models make the predictions that they do. For some domains, this can be challenging.</p> <p>Hallucinations or confabulations are the name given to a specific kind of error made by language models outputting factually incorrect information. While it's possible to reduce the hallucination rate, eliminating hallucinations entirely is a challenge. </p>"},{"location":"limitations-of-ai/#third-party-tools","title":"Third Party Tools","text":"<p>The use of third party tools like ChatGPT are increasingly common as part of scientific research.</p> <p>Take care when uploading data to a third party tool. Information that you upload may be stored by them and used for model training or other purposes. Data protection laws like GDPR may be applicable when dealing with personal data, but also consider the risk of uploading unpublished data or work to a third party service. Many services have an option to opt-out of them storing your data, so be sure to check the settings you are using. There\u2019s a need to be transparent about your use of AI with research participants, and inform them that their data may be shared with a third party. </p>"},{"location":"limitations-of-ai/#risks-concerns-and-scientific-integrity","title":"Risks, Concerns and Scientific Integrity","text":"<p>There are also a number of wider risks and concerns about the field of AI.</p> <p>ML models learn from the data they are trained on, which means that they learn and reflect any bias that was present in the training data. This can be particularly harmful if the models were trained on data scraped from the internet, which contains societal biases - like racial and gender - that shouldn\u2019t be propagated.</p> <p>The environmental cost of building and using ML models is in the spotlight. These models require large amounts of computing power to build and use, and hence use significant amounts of energy. Reducing the carbon footprint of AI is top of many people's minds.</p> <p>Due to the mistakes that ML models make, it's important to think about accountability and human oversight. Leaving ML models to make critical decisions with no oversight raises the question about who remains accountable and responsible when the system makes the wrong decision, especially in tasks where a wrong decision can cause harm. Some models are used in a human-in-the-loop scenario, where the AI model provides a prediction or decision that is checked by a human rather than acting autonomously.</p> <p>From a scientific research perspective, ensuring reproducibility is important. However, in the fast-paced field, reproducibility can be neglected. It is hard to explain the decisions of ML models, and in many fields explainability and transparency are also important. For example, a model making a prediction of medical diagnosis might also need to provide an explanation of how it reached its prediction.</p> <p>ML models are trained on data, and it's very important to know the source of data. Much data is copyrighted, and we may need consent from the owners of that data in order to use it.</p> <p>As with all technology, there is a potential for misuse. For researchers, the idea that their work can be misinterpreted and misused might seem like a far-off concern. However, it's important to consider at the outset how your work might be used in ways you don't support.</p>"},{"location":"limitations-of-ai/#policies-and-regulations","title":"Policies and Regulations","text":"<p>Policies and regulation for AI are being developed, and this is a shifting area. Always do your own research into specific policies and regulations that might apply to your research. Those might come, for example, from your funding source, your research institution, your department, or your government.</p> <p>As with other research, AI research may need to go through an ethics review board to be approved. Specific issues to be aware of include the dual use nature of AI, the use of data (including personal data), the use of AI tools in a way that goes against good scientific research practices, the proprietary nature of some tools and the lack of transparency, the bias and errors present in AI output, the ability to create misinformation at scale, and the ability to cause harm.</p> <p>GDPR is the UK\u2019s data legislation which may apply to the data you are working with. GDPR requires that you are transparent with the people you are collecting data from, that you store data securely, and that you minimise the amount of data and purpose for its use.</p> <p>There are several regulations around the globe that are relevant to AI. The first law that is specific to AI is the EU's AI act. This regulation has some banned uses of AI. It also carves out some high-risk applications that come with obligations if they are deployed. Most of the act doesn't apply to research, but could come into force if you commercialise your work. Other jurisdictions are crafting laws and guidelines that may come into force, so be sure to check the current situation as you are carrying out your research. </p>"},{"location":"limitations-of-ai/#resources","title":"Resources","text":"<ul> <li>The ICO\u2019s GDPR Guidance</li> <li>UKRI's GDPR guidance for researchers</li> <li>UK Government advice on Generative AI (aimed at civil servants)</li> <li>EU Guidelines on the responsible use of Generative AI in Research</li> </ul>"},{"location":"limitations-of-ai/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"next-steps/","title":"Next Steps","text":""},{"location":"next-steps/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>What help and support is available</li> <li>Recommended textbooks</li> </ul>"},{"location":"next-steps/#accelerate-science-resources","title":"Accelerate Science Resources","text":"<p>Now you have an idea how and where AI is used, and hopefully can identify some places in your research where you see the potential for concrete applications of AI.</p> <p>Accelerate Science provides resources to support you in your next steps:</p> <ul> <li>Our Resource hub contains links to more in depth lessons and tutorials We run several day-long workshops, including on the Generative AI topics of LLMs and Diffusion models</li> <li>We provide AI &amp; Software Engineering help to researchers via our AI Clinic, where you can file a ticket to receive one-to-one help</li> <li>We run regular drop-in AI Cafe sessions where you can turn up and talk with an expert - look out for the next one, or get in touch to organise with us</li> <li>Our introduction to Python Programming course will support you as you get started with learning python</li> <li>We run courses on publishing and packaging software and Docker, to help you with the software practicalities of running AI projects</li> </ul>"},{"location":"next-steps/#where-else-can-i-look","title":"Where else can I look?","text":"<p>There are many free resources online to learn about AI. We also recommend this textbook for those looking for code examples of some of the concepts discussed so far:</p> <p>Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow : concepts, tools, and techniques to build intelligent systems / Aur\u00e9lien G\u00e9ron.</p> <p>A second textbook is this one, which dives deeper into some of the ideas presented here:</p> <p>Deep Learning: Foundations and Concepts / Chris Bishop &amp; Hugh Bishop</p>"},{"location":"next-steps/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"nlp/","title":"Natural Language Processing","text":""},{"location":"nlp/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>What falls under the umbrella of Natural Language Processing (NLP)</li> <li>Some common NLP tasks</li> <li>Examples of NLP applied to scientific research</li> </ul>"},{"location":"nlp/#what-is-nlp","title":"What is NLP?","text":"<p>Natural Language Processing (NLP) is the umbrella term for techniques that involve processing text data. NLP is a large domain, and there are many opportunities to use NLP across a wide range of scientific disciplines. NLP techniques that are used in one domain are very easily transferred to another with just a change in the data being processed. A technique which works well on legal texts, for example, is likely to work well on medical text data. Or a model that has been trained on general text data can be tuned to work with scientific papers. Hence, if you anticipate processing text data in your research, it can be valuable to look to the NLP literature to understand what has been done in similar domains.</p> <p>The field of NLP has been transformed by AI and machine learning. Supervised and unsupervised learning techniques have been applied across a range of text processing tasks in many different disciplines. The NLP field has had a key influence in transformer-based foundation models and generative AI.</p>"},{"location":"nlp/#nlp-tasks","title":"NLP Tasks","text":"<p>If you want to work with text data in your field, the chances are good that you'll want to use some kind of NLP technology. Many common NLP tasks exist, which have a long line of research into them, including:</p> <ul> <li>Language Modelling is the task of predicting the next word, like an autocomplete. It's the basis of recent progress in NLP relating to generative AI.</li> <li>Machine Translation automatically translates text from a source language to a target language.</li> <li>Summarisation is when a model is used to summarise a long piece of text into a shorter one.</li> <li>Question Answering answering questions where the answer can be found in a text that has been provided. E.g. answering questions from a given textbook.</li> <li>Sentiment Analysis analysing whether a piece of text is positive or negative in its sentiment.</li> <li>Named Entity Recognition picking out certain key entities from a phrase. For example, picking out names of molecules, drugs or diseases from text.</li> <li>Text classification assigns predefined categories to text documents such as genre of a paper.</li> <li>Coreference resolution identifies which references to people or objects in a text are referring to the same entity, such as when \u201cit\u201d and \u201cthe paper\u201d are referring to the same item</li> <li>Semantic similarity is identifying whether two pieces of text have similar meanings</li> <li>Relation extraction identifies relations between two entities in text, like \u201cParis is a city\u201d or \u201cthe apple is green\u201d.</li> <li>Word sense disambiguation identifies which sense, or definition, of a word is being used, e.g. whether \u201cbank\u201d refers to a river bank or the financial institution.</li> </ul> <p>Many of these tasks have well established benchmark datasets, which researchers use to evaluate the performance of these models in different domains.</p>"},{"location":"nlp/#examples-of-nlp-in-scientific-research","title":"Examples of NLP in Scientific Research","text":"<p>NLP models are typically trained using text data from the internet. This data is often selected to have a broad coverage of general topics. Researchers wanting to use NLP techniques in a specific domain or field may need to collect text from that domain for their models, to capture the kinds of vocabulary and phrases that are used.</p> <p>There are many sources of text data available to researchers. Examples of NLP work on data sources that researchers might be interested in include:</p> <ul> <li>NLP techniques help researchers search through scientific literature, provide summaries, answer questions about papers and find new papers that are similar to existing papers. </li> <li>Language models can be built using electronic health records, which can then be used for tasks like extracting medical concepts and question answering.</li> <li>NLP models monitor changing sentiment about events such as covid-19 from social media data</li> <li>NLP can be used to analyse legal texts and policy documents, providing researchers with key points about those policies</li> </ul>"},{"location":"nlp/#resources","title":"Resources","text":"<p>NLP &amp; generative AI are covered in our 1-day workshop on Large Language Models. The material and associated code are available online.</p>"},{"location":"nlp/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"practical/","title":"Some Practical Advice","text":""},{"location":"practical/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>Programming for implementing AI</li> <li>Managing experimentation</li> <li>Ways to share your work</li> </ul>"},{"location":"practical/#overview","title":"Overview","text":"<p>As AI has progressed, there are an increasing number of AI tools that are available to researchers. General tools like ChatGPT are generally available, while specific research tools for tasks like segmenting images &amp; searching scientific papers are being integrated into research workflows. Many researchers are moving beyond using pre-packaged tools and experimenting with applying AI to their own research field. </p>"},{"location":"practical/#computer-programming","title":"Computer Programming","text":"<p>If you choose to apply AI in your own research, you are going to need to learn some programming. Even if you don't intend to train your own models, but use models that have already been built by other researchers, you may find that programming is helpful to speed up your work.</p> <p>Most AI and ML work is done in Python. In some fields, R is used as an alternative. Python has great support for machine learning. There are many open-source software libraries available to use, such as scikit-learn, Pytorch and HuggingFace\u2019s transformers. Software libraries are pre-written code that are available for you to use in your own work.</p> <p>Many researchers begin exploring ML code using Notebooks such as Jupyter or Google Colab. These can be a great way to get started exploring, but you might find that once you've made some good progress your code becomes unwieldy and you look for other ways to structure your work.</p> <p>Managing the code for your project is best done using a version control tool like GitHub. A tool like this will allow you to track the changes you make to your code, go back and forth between versions, and easily collaborate with colleagues.</p> <p>Remember that software comes with licences, and you must check the licence of any software you use to be sure you have the correct permissions to use it. Familiarise yourself with different software licences and their implications for using, modifying &amp; distributing your own software. </p>"},{"location":"practical/#open-source","title":"Open Source","text":"<p>There is a large open source AI community, with a practice of sharing models, data and code. These communities not only share resources, but also have community forums for support. It is highly recommended to explore the open source landscape before writing your own code and building your own models. It can save a lot of time and effort to build on existing work than to start from scratch. However, there is a wide range of quality in open source software. Projects that are actively maintained are generally more reliable.</p> <p>HuggingFace are an organisation that host a large repository of datasets and models that have been open sourced. It's a great place to start looking for data and models that are relevant to your task.</p>"},{"location":"practical/#good-experimental-practice","title":"Good Experimental Practice","text":"<p>Once you've got started with programming, the next challenge is usually managing experiments and tracking work.</p> <p>AI and Machine Learning projects are iterative, and typically you end up trying a lot of different models, hyperparameters, data sets and pre-processing pipelines. Modern AI libraries (like pytorch and scikit-learn) have easy-to-use implementations of many techniques, meaning that it\u2019s easy to try multiple ideas. However, this means it can be easy to rapidly create lots of files and find your experiments hard to navigate.</p> <p>Experiment tracking tools like Weights and Biases, or MLflow, can provide ways to simplify the experiment structure. </p> <p>For recording your work, Model Cards and Data Sheets provide a structured way to log what you've built so that others can understand and build on your work. </p>"},{"location":"practical/#sharing-your-work","title":"Sharing your work","text":"<p>At the end of your project, sharing your model, data and code with the open source community is a possibility. Openness is key to scientific rigour, and published software is a recognised research output which can help with your career progression.</p> <p>Accelerate Science have a course on publishing and packaging software, which you can sign up to, or follow the materials online at your own pace.</p>"},{"location":"practical/#theory-in-depth","title":"Theory in-depth","text":"<p>If you decide to go further and want to understand the theory of AI in-depth, you\u2019ll want to explore mathematical topics such as linear algebra, optimisation, probability and calculus. With some understanding of these topics, you\u2019ll be able to dive into the theory of AI and ML, and you\u2019ll have a foundation for designing your own models and algorithms.</p>"},{"location":"practical/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"reinforcement-learning/","title":"Reinforcement Learning","text":""},{"location":"reinforcement-learning/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>What reinforcement learning is</li> <li>Some real-world examples of reinforcement learning in research</li> </ul>"},{"location":"reinforcement-learning/#what-is-reinforcement-learning","title":"What is Reinforcement Learning?","text":"<p>Reinforcement learning is a type of machine learning aimed at tasks where the machine, or agent, has to take a series of moves in an uncertain and changing environment, to achieve a specific goal. The agent learns a strategy, or policy, that guides how to take actions based on the current state of the agent within the environment.</p> <p>A typical application of reinforcement learning is in playing games. Some of the biggest breakthroughs in the field have come through playing board games like Go, or video games like StarCraft. Reinforcement learning models optimise a reward. In gaming that reward would be high if they win the game, and low if they lose. In gaming, the environment is uncertain because there are multiple players, and it\u2019s not possible to know what moves each player will make. It\u2019s also not possible to know the winner of the game, and hence the final reward, until the game has completed. Thus, the correct choice of action by the agent at any time must take into account indirect and delayed consequences of that action in the future, and hence requires a form of planning.</p> <p>Another example of reinforcement learning is for controlling robots in a lab environment. Here, the sensors on the robot have some uncertainty in their measurements, and the lab environment may change based on what people in the lab are doing. The reward for the robot is high if they successfully complete their task, and low if they fail.</p> <p>Unlike supervised and unsupervised learning, which learn their behaviour from fixed datasets, reinforcement learning agents learn through repeated interactions with the environment. Agents begin with random behaviour, and through training in a trial-and-error fashion they learn to exhibit the desired behaviour. Often it is too time-consuming and expensive to learn entirely through direct interaction, and so a simulated environment is often used in training. This is why video games are a great testbed, as they are inherently a simulation.</p> <p>During learning, reinforcement learning algorithms rely on a mixture of exploration, or trying new actions to discover what works, and exploitation, which is trying actions that have already been proven to work in order to maximise the reward.</p> <p>In practice, reinforcement learning is used much less than supervised and unsupervised learning as it is more complicated to implement and to achieve good results.</p>"},{"location":"reinforcement-learning/#reinforcement-learning-use-cases","title":"Reinforcement Learning Use Cases","text":"<p>RL is used in scenarios like:</p> <ul> <li>Playing games, where the computer can choose to make optimal moves but there is some uncertainty about the moves its opponent may make.</li> <li>Controlling robots in real-world environments such as the lab, where there is uncertainty about how the environment changes over time.</li> <li>Identification of novel drug candidates. In this task, actions available to the agent are modifications to molecules, and the reward function provides feedback about desired properties of the molecules. </li> <li>In quantum Physics, reinforcement learning has been applied to the problem of finding sequences of commands for manipulating the states of a quantum system in a controlled manner. </li> </ul> <p>In each of these tasks, it\u2019s hard to judge in isolation whether a single turn or move by the computer is optimal. Yet, we do know in the long-term whether the computer achieves its goal.</p>"},{"location":"reinforcement-learning/#inspiration","title":"Inspiration","text":"<ul> <li>One famous example of reinforcement learning is Deepmind's AlphaGo model, that learned to play the game Go</li> </ul>"},{"location":"reinforcement-learning/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"supervised-learning/","title":"Supervised Learning","text":""},{"location":"supervised-learning/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>What supervised learning is, and where you can use it</li> <li>The difference between classification and regression</li> <li>Some of the supervised learning algorithms you might encounter</li> <li>Some real-world examples of supervised learning in research</li> </ul>"},{"location":"supervised-learning/#what-is-supervised-learning","title":"What is supervised learning?","text":"<p>Supervised learning is the most widely used type of machine learning algorithm, and it\u2019s been widely applied across scientific fields. For example, in Chemistry it\u2019s been used to predict the outcomes of reactions, in Materials Science to predict properties of new materials, in Climate Science for weather forecasting, in Biology for identifying cell types, and in Medicine for segmenting and classifying medical images.</p> <p>Supervised learning has a broad range of applications in science, providing powerful tools for prediction, classification, and analysis. It enables researchers to extract insights from complex datasets, driving advancements across scientific disciplines.</p> <p>Supervised models make predictions about their inputs. They learn how to make those predictions from a dataset of examples that are labelled with the correct prediction (also called the ground truth). One downside of supervised learning is that the dataset of examples can be expensive to obtain, especially if a person has to take the time to go through the data and manually annotate each example with its label.</p> <p></p> <p>When designing a supervised learning algorithm, researchers need to define what the input and output of a model should be, to obtain the labelled data for training, and then train the model to make accurate predictions.</p> <p>One example of supervised learning is predicting whether an email is spam or not. In this example there are two categories, or classes to which an email might belong: SPAM and NOT_SPAM. A supervised learning algorithm for identifying spam emails would be trained on a dataset of emails labelled by whether they really are spam messages or not.</p> <p>Machine learning models cannot directly accept email text as input, so numerical features are first extracted from each of the emails. Two features might be, for example, the length of the email, the number of spelling mistakes.</p> <p></p> <p>By training the model directly on a dataset of emails that have been labelled as being spam or not, the model can learn the patterns that identify the two types of email, and then predict for a new email whether it is spam or not.</p>"},{"location":"supervised-learning/#classification-and-regression","title":"Classification and Regression","text":"<p>Supervised learning can either be classification or regression. You will need to choose between these two modes depending on the task you are modelling and the data you have collected.</p> <p>Classification is where the output of the model is one of a fixed number of categories. This could be a binary classification task where there are exactly two possible outputs, like the example above of spam email detection. Another example of a binary classification task is that of predicting whether a new drug candidate is toxic or not. You can think of the model as finding a line to separate the two classes, as shown here:</p> <p></p> <p>Usually, there's some overlap between the two classes, so it\u2019s not possible for a model to completely separate them. In the example above, some examples of spam email fall to the left of the line and are misclassified as non-spam, while some examples of non-spam email fall to the right and are wrongly identified as spam. This example also shows the model as a straight line separating the two classes. Typically, supervised learning models find a way to separate the classes that\u2019s more complex than a straight line.</p> <p>A different task might have multiple categories that an input could belong to. For example, recognising the face of a person in a photo. There are many possible people who could be pictured in the photo, and each person is a possible category. In Physics, a multi-class classification task could be galaxy classification, classifying the type of galaxy in an image. In this example, the classifier learns to separate three categories:</p> <p></p> <p>Sometimes the number of classes can be very large, even into the thousands or millions, as is the case with many language processing tasks. Automatically translating text from one source language to another target language is a supervised task, where the model predictions are words in the target language. As languages have many thousands of words, then the number of output classes is also in the thousands. </p> <p>In other scenarios, an input might have more than one label. For example, a photo might have two or more different people in it, and we'd like to identify each. This is called multi-label classification, because each photo has more than one label associated with it.</p> <p>Regression is similar to classification, except that the output is a continuous number rather than a discrete class. An example might be predicting the BMI of a patient based on blood test biomarkers. Fitting a line of best fit is a simple kind of regression, as in this example:</p> <p></p>"},{"location":"supervised-learning/#feature-extraction","title":"Feature Extraction","text":"<p>Machine learning models require numerical inputs, and you'll need to consider that in designing your setup. Some tasks naturally have numerical inputs, or features, already - whether that's data from sensors, medical tests, pixel values or some other kind of data that's naturally a number. In other tasks, like in text processing, we need to convert our input to numbers by extracting features.</p> <p>For the spam email task described above, we extract two features from each email, giving a 2-dimensional feature vector:</p> <p></p> <p>In a different task, in an example clinical setting, there might extract eight features about each patient, and they\u2019re combined into an 8-dimensional feature vector:</p> <p></p> <p>In other scenarios, the feature vectors might have hundreds or thousands of dimensions. Perhaps you have a sensor measuring many readings, or lengthy gene expression data in a bioinformatics task. Still, whether you have a small number of features, or many thousands, the first task is to convert it into numbers that can be input to a machine learning model. </p>"},{"location":"supervised-learning/#examples-of-supervised-learning","title":"Examples of Supervised Learning","text":"<p>Examples of supervised learning tasks include:</p> Task Input Prediction Identify and segment tumours in medical images Medical images Areas in the image corresponding to tumours Predict patient's BMI from biomarkers Biomarkers BMI Predicting whether a new drug candidate is toxic or not Chemical formula Toxicity Medical Diagnosis Biomarkers, test results etc. Diagnosis Automated transcription Audio files Written transcription Machine translation Text in the source language Text in the target language"},{"location":"supervised-learning/#types-of-supervised-learning-model","title":"Types of Supervised Learning Model","text":"<p>There are many supervised learning algorithms to choose between, and some of the common algorithms you might encounter are:</p> <ul> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Linear Discriminant Analysis</li> <li>Decision Trees </li> <li>Support Vector Machines</li> <li>Random Forests</li> <li>Neural Networks</li> </ul> <p>Much of the recent excitement around AI focuses on neural network approaches. However, depending on your data and task, the other algorithms may be more practical to use. The choice of which algorithm to use will depend on several factors such as how much data you have, how complex your task is, and how much computational power you have available. In practice, researchers may try several different algorithms, starting with the simplest, and compare their performance to discover which works best. </p>"},{"location":"supervised-learning/#case-study","title":"Case Study","text":"<p>Healthcare; Dr Christopher Bannon</p> <p>Obesity is a risk factor for conditions like type-2 diabetes, and scientists have been using AI methods to help understand the effect of obesity on the body. In particular, understanding the effect of specific hormone levels can open up the path to better understanding of the biology of obesity.</p> <p>This study had a group of 205 participants in obese and non-obese groups. Participants in both groups fasted overnight, and then were given a standardised meal. Levels of two hormones from the gut (PPY and GIP), and two pancreatic hormones (insulin and glucagon), were measured from blood samples using a technique called immunoassay. The hormones were measured before and after eating, giving 8 readings in total.</p> <p>A supervised learning algorithm - Linear Regression - was used to predict body mass index (BMI) from these hormone level readings. With a set of 205 participants, the number of features was chosen to be small and avoid overfitting. 4 hormone levels were selected from the 8 measured, and two different regression models were built - one for obese, and one for non-obese, participants.</p> <p>One advantage of linear regression, as compared to more complex supervised learning algorithms, is that it's possible to look inside the model and examine the impact that each feature has on the prediction. By using linear regression to predict BMI from hormone levels, it's feasible to see the contribution of each hormone to the BMI prediction. Results showed that gut hormones PYY and GIP, and pancreatic hormones insulin and glucagon, showed strong relationships with BMI in non-obese subjects. In obese participants however, the major BMI correlations were with pancreatic hormones only.</p> <p>Paper: https://www.sciencedirect.com/science/article/pii/S0196978124000391</p>"},{"location":"supervised-learning/#inspiration","title":"Inspiration","text":"<p>Find more examples of research using supervised learning on Accelerate's blog:</p> <ul> <li>Nicola Moloney talks about using supervised learning to predict where proteins are localised within a cell</li> <li>Joyce Nakatumba-Nabende discusses how speech recognition can be deployed to help farmers in Uganda</li> <li>Yizhou Wan tells us how AI can be used to segment brain tumours in images, and to estimate their volume</li> <li>Chris Bannon hopes to predict whether someone's gut biome or metabolic marker levels are that of a healthy person, or whether they have a metabolic or bowel condition</li> <li>Dr Toby Jackson uses supervised learning to identify trees from satellite images, which can be used to track progress against the city\u2019s goal to increase tree cover </li> </ul>"},{"location":"supervised-learning/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"training/","title":"Training ML Models","text":""},{"location":"training/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>What it means to train an AI model</li> <li>Hyperparameters</li> <li>Issues with training like over- and under-fitting</li> <li>Transfer learning and fine tuning</li> </ul>"},{"location":"training/#model-training","title":"Model Training","text":"<p>Model training is at the heart of an AI project. It\u2019s the process of feeding data examples to your model so that it can learn the patterns that are present. In supervised learning and generative AI, the model learns from data examples and their labels. In unsupervised learning, the model learns just from the data examples with no labels. In reinforcement learning, the model learns through interaction with the real world, or a simulation of it.</p> <p>Models themselves consist of parameters, or weights, and training is the process of learning the values of those parameters. A linear regression model trained on one dataset might have the same number of parameters as that trained on another, but the learned parameter values are different because they've been learned from the different training datasets.</p> <p>The number of parameters varies wildly between models. A small linear regression model might have in the order of 10 parameters, while a state-of-the-art generative AI model might have 10 billion or more parameters. Larger models require more time &amp; computational power to train - often needing multiple GPUs for the largest models - while small models can be trained locally in minutes on just a CPU. Complex tasks typically need larger models for acceptable performance.</p> <p>The training algorithm itself to learn these parameter values is also an iterative process. You feed training data into the model, then the algorithm figures out how wrong the model is for these data points and makes small adjustments to the parameters to perform a little better. The question of whether the model fits the data points is judged by a loss function or an objective function that can compute how well the model matches the data as training progresses. Over time, with lots of iterations passing over the training dataset, the parameters converge to a place where they model the training data well. Each pass over the entire training data set is known as an epoch.</p> <p>As a researcher using AI, you can often get started exploring AI without needing to know the details of the training, as there are powerful software libraries that handle the implementation.</p>"},{"location":"training/#hyperparameters","title":"Hyperparameters","text":"<p>The purpose of model training is to find optimal values for the model parameters. However, there are other parameters in machine learning that aren't part of the model. These are called hyperparameters, and they affect the outcome of the model training. These might be, for example, the number of layers in your neural network, the number of training epochs, or the learning rate of your training algorithm.</p> <p>Many implementations of AI model training have good default settings for the hyperparameters. However, one purpose of a development set is to have data available for tuning the hyperparameters. There are several strategies for hyperparameter tuning. The simplest is known as grid search, and involves testing hyperparameter values across a range of values. So, in practice, tuning the hyperparameters means building models with several different hyperparameter values, testing the performance of those models on the development data, and choosing the model with best performance. By tuning hyperparameters on a development set, the test set can remain held out, and avoid overfitting.</p>"},{"location":"training/#randomness-in-training","title":"Randomness in Training","text":"<p>There are many places in ML training where randomness is used. Examples include the random initialisation of weights (parameters) at the beginning of training, the hardware that you\u2019re building on, or the random shuffling of the data that you\u2019re using for training. Randomness is essential to the performance of ML models, but can also prevent reproducibility of your work. </p> <p>Two ways to account for randomness are:</p> <ul> <li>Use a fixed \u2018random seed\u2019 to get the same results each time your code is run, and enable reproducibility</li> <li>Train multiple models with multiple random settings, and average the results</li> </ul> <p>Randomness can be very difficult to eliminate entirely in more complex experimental setups. </p>"},{"location":"training/#issues-with-training","title":"Issues with training","text":"<p>Two ways that training can be derailed are over- and under-fitting.</p> <p>Over-fitting is where the model learns the training data too well, capturing noise and details that do not generalise to the test data. This results in poor performance on development or test data, despite excellent performance on the training data. Over-fitting can be a risk if you don\u2019t have enough data, or your model is complex.</p> <p>Under-fitting is the opposite, where a model is too simplistic to capture details of the training data. You might have a lot of data and a simple model, so the model isn't able to approximate the data well. In this case the model will perform poorly on both your training and test set.</p>"},{"location":"training/#training-vs-finetuning","title":"Training vs Finetuning","text":"<p>You do not always need to train a model from scratch. Often, there can be a model out there which you can use as a starting point. Then, you would finetune that model to your data, or use transfer learning.</p> <p>Finetuning is where you keep the model architecture as it is, and just train the model a bit more on your data.</p> <p>Transfer learning is similar to finetuning, but with the understanding that you're training the model for a different task than the one it was originally trained to do. With neural networks, this typically means replacing the last layer with one which is designed for your task.</p> <p>The advantage of starting from an existing pretrained model is that you typically don't need as large a dataset as you would if training a model entirely from scratch. </p>"},{"location":"training/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"unsupervised-learning/","title":"Unsupervised Learning","text":""},{"location":"unsupervised-learning/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>What unsupervised learning is</li> <li>Unsupervised learning algorithms including clustering, anomaly detection and dimensionality reduction</li> <li>Some real-world examples of unsupervised learning in research</li> </ul>"},{"location":"unsupervised-learning/#what-is-unsupervised-learning","title":"What is Unsupervised Learning?","text":"<p>Most data in the world is unlabelled. Unsupervised learning helps make sense of unstructured and unlabelled data. It\u2019s been used across scientific fields, for example, in Medicine discovering sub-groups of participants in medical studies, in Physics identifying rare astrophysical events, and in Archeology grouping archeological sites to determine which might be related to each other.</p> <p>Unsupervised learning is invaluable in scientific research for discovering hidden patterns, segmenting data, and reducing complexity in large datasets. It allows scientists to make sense of vast amounts of data and generate hypotheses that can be tested in further research.</p> <p>Three common unsupervised algorithms are clustering, dimensionality reduction and anomaly detection. One or more of these algorithms may be helpful to you if you have unlabelled data to explore. </p>"},{"location":"unsupervised-learning/#unsupervised-learning-algorithms","title":"Unsupervised Learning Algorithms","text":"<p>Three common unsupervised algorithms are clustering, dimensionality reduction and anomaly detection.</p> <p>Clustering</p> <p>Clustering is a technique that helps uncover structure in your data by discovering sub-groups within a larger dataset. It\u2019s a technique that may be useful to explore if you believe that your data contains different sub-groups, and you\u2019d like to discover them.</p> <p>Across disciplines, clustering finds many applications. For example, you may be working with medical data about a specific treatment, and you notice that some people respond differently than others. If you don\u2019t know the factors that drive this difference, then clustering may help you uncover different sub-groups that respond in different ways to the treatment. In Biology it can be used to group cells and identify distinct types of cell in the body. In the social sciences, individuals in behavioural studies can be grouped based on their behaviour. In Materials Science, materials can be grouped according to their similar behaviour in specific scenarios. </p> <p></p> <p>Unlike classification in supervised learning, where we know the groups ahead of time, clustering uncovers the unknown structure of sub-groups in a larger population. Clustering does not name the groups it uncovers, or provide any interpretation. Researchers must themselves examine and interpret the clusters to determine whether they are valid and useful, perhaps even by using the results to generate new research hypotheses to test experimentally.</p> <p>There are many different clustering algorithms available to use. Two of the most popular and well-used are k-means clustering and DBSCAN. These are great starting points for any researcher interested in exploring clustering on their data. </p> <p>Dimensionality reduction</p> <p>High dimensional data - ie data with a large number of features - is very common in scientific fields but is hard to work with. High dimensional data can come from different sources. Sensor data, weather data, large hadron collider data, spectroscopy, even survey data in the social sciences, can all generate many hundreds or thousands of features. Dimensionality reduction techniques are typically used in two ways - to visualise data, and to reduce the number of dimensions before applying another algorithm like clustering or supervised learning.</p> <p>It is difficult to visualise high-dimensional data well so that you can explore what it looks like. Reducing the dimensionality of your data is one way to visualise it effectively. Projecting down from tens, hundreds or thousands of dimensions to 2- or 3-dimensions means that you\u2019re able to plot your data on a graph to view.</p> <p></p> <p>Principal Component Analysis (PCA), t-SNE, and UMAP are popular dimensionality reduction algorithms that you might see used for visualisation of high-dimensional data.</p> <p>A high number of dimensions also makes using algorithms like supervised learning and clustering difficult. High dimensionality data requires you to build models with a large number of parameters, to effectively model your data. The more parameters a model has, the more data you need. This is known as the curse of dimensionality. High dimensional data necessitates large datasets. </p> <p>Dimensionality reduction algorithms are often used on high-dimensional data to reduce the number of dimensions, before then using another algorithm like clustering or supervised learning. This low-dimensional representation of a higher-dimension space is called the latent space. For example, in Astrophysics, PCA has been used to reduce the dimension of spectroscopy data, before doing supervised classification to determine the type of stellar object. In Biology, PCA can be used to reduce the high-dimensional gene expression data, which can be thousands of features, to a low-dimensional latent space before doing further experiments.</p> <p>High-dimensional datasets are common across scientific disciplines, and dimensionality reduction techniques allow researchers to reduce the complexity of their data to work with it more effectively. </p> <p>Anomaly Detection</p> <p>Anomaly detection is identifying anomalous or rare examples in a dataset that are inconsistent with the rest of the dataset. Identifying rare events can be valuable in domains that generate lots of data, where you\u2019re looking for a \u2018needle in a haystack\u2019, and doing so manually is very time-consuming. These rare events can then become the subject of future research. They can also be valuable when timely identification of an unusual event is crucial, like ahead of a natural disaster, when automated methods can be faster than a person.</p> <p></p> <p>Scientific applications of anomaly detection include:</p> <ul> <li>Astrophysics - detecting short-lived events (known as transients) such as supernovae, or identifying scientifically interesting objects like exoplanets </li> <li>Climate Science - identifying rare weather events or giving early signs of natural disasters such as predicting earthquakes from seismic data, or wildfire risk from weather patterns</li> <li>Medicine - identifying abnormalities in lab results e.g. triaging patients based on CT brain scans, or identifying malignant tumours as outliers in medical images</li> </ul> <p>Anomaly detection can help researchers identify significant, unusual events that can lead to important discoveries, and provide early warnings for other scenarios. </p>"},{"location":"unsupervised-learning/#inspiration","title":"Inspiration","text":"<p>Find more examples of research using unsupervised learning on Accelerate's blog:</p> <ul> <li>Ema Bauzyte discusses using unsupervised learning to cluster sites in archeology, get more insight about which might be related</li> <li>Jesse Allardice uses dimensionality reduction in Physics, to reduce the complexity of high dimensional data for solar panels</li> <li>Dr Romit Samanta groups patients to better identify subgroups of patients and understand how their biology relates to disease progression</li> </ul>"},{"location":"unsupervised-learning/#contact","title":"Contact","text":"<p>If you can't find what you need</p> <p>CONTACT US </p>"},{"location":"what-is-ai/","title":"What is AI?","text":""},{"location":"what-is-ai/#learning-objectives","title":"Learning Objectives","text":"<p>This section will help you understand:</p> <ul> <li>What AI is and where it is used</li> <li>Some different definitions of AI and related terms</li> </ul>"},{"location":"what-is-ai/#overview","title":"Overview","text":"<p>Artificial Intelligence (AI) is an umbrella term. Usually it\u2019s thought of as something like imitating human intelligence. AI systems are used to automate decision making in tasks that typically need some level of intelligence to perform.</p> <p>When people talk about AI today, they're almost always talking about machine learning (ML). All of the recent progress in the field of AI has been in machine learning, and particularly in a specific type of machine learning called deep learning. Machine Learning is a set of algorithms that learn their behaviour from data. This is in contrast to more traditional computer programming that spells out the rules that a computer must follow.</p> <p>There are 4 broad categories of ML that these pages explore:</p> <ul> <li>Supervised Learning: The model is trained on labelled data to make predictions or classify new data, such as predicting disease from medical images.</li> <li>Unsupervised Learning: Uncovers structure, patterns and relationships in unstructured and unlabelled data, such as identifying clusters of cells based on their gene expression in genomics.</li> <li>Reinforcement Learning: Builds agents that learn to take actions in uncertain and changing environments, such as controlling robot automation in the lab.</li> <li>Generative AI: The model creates new content, such as identifying new molecular structures in drug discovery. </li> </ul> <p>These pages explore each of these areas and how they are being applied in AI for science projects.</p>"},{"location":"what-is-ai/#what-is-ai_1","title":"What is AI?","text":"<p>AI models are built to tackle specific tasks. A task is something specific that a model might do such as:</p> <ul> <li>Make a decision about whether an email is spam or not</li> <li>Identify people in images</li> <li>Translate text from one language into another</li> <li>Automatically transcribe speech</li> <li>Identify supernovae in Astrophysics data</li> <li>Predict whether an X-ray shows a cancerous mass</li> <li>Predict the toxicity of a chemical compound</li> <li>Classify the type of cell from gene expression data</li> <li>Identify scientific papers that are close in topic to each other</li> </ul> <p>Tasks usually have datasets associated with them. A dataset is essentially lots of examples of that task. For example, a spam email detection dataset would consist of many emails. A speech recognition dataset would consist of audio of people speaking. A medical image dataset would consist of many examples of medical images.</p> <p>A dataset might have labels (or annotations), which are the correct answer, for each item in the dataset. The emails in your spam email dataset would have labels marking each email as spam or not. The audio files in your speech recognition dataset would have accurate transcriptions of the words spoken. The medical images dataset might contain information about suspicious masses in the images. These labels are often created by people who manually go through the dataset and annotate each example.</p> <p>Many popular AI tasks have open source datasets that are available to use. Though, depending on the existing research in your field, you might need to acquire your own dataset.</p> <p>An ML model is trained to perform a task, using one or more datasets. The training process updates the model\u2019s parameters, or weights. These are the set of numbers inside the model which are learnt from data. The model is usually the artefact that's the result of your experimentation, and it can be used again and again on new data.</p> <p>You need to evaluate the performance of your model based on a subset of your data, to judge how well your model achieves the task you intend.</p> <p>Under the umbrella of ML, there are many different algorithms. Deep Learning, or Deep Neural Networks, are popular today because they\u2019ve been successful at a range of different tasks. There are many other algorithms available though, some of which may be more suitable for your data.</p>"},{"location":"what-is-ai/#ai-for-scientific-research","title":"AI for Scientific Research","text":"<p>Researchers across disciplines are exploring the application of AI to data and tasks in their own domains. AI is being used across the entire scientific research lifecycle:</p> <ul> <li>Speeding up literature review, by using text processing techniques to sift through large amounts of scientific text.</li> <li>Hypothesis generation, by uncovering relationships and patterns that may not be immediately apparent to human researchers.</li> <li>Data analysis, by identifying patterns in data.</li> <li>Simulation and modelling, by simulating complex processes &amp; providing insights into phenomena that are difficult or impossible to study experimentally.</li> <li>Writing assistant, by helping suggest ways to write scientific findings.</li> </ul> <p>The following sections dive into different types of AI algorithms and highlight how they work in scientific research, along with some of their limitations.</p>"}]}